{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBtyHGgkgUz4"
      },
      "outputs": [],
      "source": [
        "!pip3 uninstall gym\n",
        "!pip3 install box2d-py\n",
        "!pip3 install gym[box_2D]\n",
        "!pip3 install gym[box2d]\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from PIL import Image as im\n",
        "\n",
        "import datetime\n",
        "from statistics import mean\n",
        "from gym import wrappers\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(Model): \n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define the layers.\n",
        "        self.conv_1 = tf.keras.layers.Conv2D(filters = 16, kernel_size = 3,\n",
        "                                               activation=tf.keras.activations.relu, input_shape = (96,96,3)\n",
        "                                               )\n",
        "        self.max_pool_1 = tf.keras.layers.MaxPool2D()\n",
        "        self.conv_2 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3,\n",
        "                                               activation=tf.keras.activations.relu\n",
        "                                               )\n",
        "        self.max_pool_2 = tf.keras.layers.MaxPool2D()\n",
        "\n",
        "        self.global_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "        self.output_layer = tf.keras.layers.Dense(3,\n",
        "                                                  activation = tf.keras.activations.sigmoid\n",
        "                                                  )\n",
        "\n",
        "    def call(self, x):\n",
        "        # Define the forward step.\n",
        "        x = self.conv_1(x)\n",
        "        x = self.max_pool_1(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.max_pool_2(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def sample_prob(self, state, action = False):\n",
        "        \n",
        "        # get network output for one state\n",
        "        mu = self(state)\n",
        "        # rescale first action to be between -1 and 1 instead of 0 and 1\n",
        "        mu = tf.convert_to_tensor([[(mu[0][0]*2)-1,mu[0][1],mu[0][2]]])\n",
        "\n",
        "        # if there is no action given, sample one\n",
        "        if not action:\n",
        "            steering = tf.random.normal([1], mu[0][0], 0.5) \n",
        "            gas = tf.random.normal([1], mu[0][1], 0.25)\n",
        "            breaking = tf.random.normal([1], mu[0][2], 0.25)\n",
        "\n",
        "            action = [steering.numpy()[0], gas.numpy()[0], breaking.numpy()[0]]\n",
        "        \n",
        "        log_prob = tfp.distributions.Normal(mu, [0.5, 0.25, 0.25]).log_prob(action)\n",
        "        return log_prob, action\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Critic(Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define the layers.\n",
        "        self.conv_1 = tf.keras.layers.Conv2D(filters = 16, kernel_size = 3,\n",
        "                                               activation=tf.keras.activations.relu, input_shape = (96,96,3)\n",
        "                                               )\n",
        "        self.max_pool_1 = tf.keras.layers.MaxPool2D()\n",
        "        self.conv_2 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3,\n",
        "                                               activation=tf.keras.activations.relu\n",
        "                                               )\n",
        "        self.max_pool_2 = tf.keras.layers.MaxPool2D()\n",
        "\n",
        "        self.global_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "        self.output_layer = tf.keras.layers.Dense(1,\n",
        "                                                  activation = tf.keras.activations.linear\n",
        "                                                  )\n",
        "\n",
        "    def call(self, x):\n",
        "        # Define the forward step.\n",
        "        x = self.conv_1(x)\n",
        "        x = self.max_pool_1(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.max_pool_2(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "soKuRcdigcQ3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_state(state):\n",
        "    # state = np.dot(state[..., 0:3], [0.299, 0.587, 0.114])\n",
        "    state = tf.cast(state, tf.float32)\n",
        "    state /= 255\n",
        "    state = tf.expand_dims(state, axis = 0)\n",
        "    return state"
      ],
      "metadata": {
        "id": "4fEaWopxvCtQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_trajectories(model, env, defined_steps = False):\n",
        "  \n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  state = preprocess_state(state)\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  next_states = []\n",
        "  log_probs = []\n",
        "  i = 0\n",
        "\n",
        "  while not done and (not defined_steps or defined_steps > i) : \n",
        "    log_prob, action = model.sample_prob(state)\n",
        "\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    next_state = preprocess_state(next_state)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    next_states.append(next_state)\n",
        "    log_probs.append(log_prob)\n",
        "    state = next_state\n",
        "\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  return states, actions, rewards, next_states, log_prob"
      ],
      "metadata": {
        "id": "pQZUUWkEZqa0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vanilla Policy Gradient\n",
        "actor = Actor()\n",
        "critic = Critic()\n",
        "env = gym.make(\"CarRacing-v2\")\n",
        "\n",
        "episodes = 10\n",
        "gamma = 0.99\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "for i in range(episodes):\n",
        "  states, actions, rewards, _, _ = create_trajectories(actor,env,100)\n",
        "\n",
        "  sum_reward = 0\n",
        "  discnt_rewards = []\n",
        "  rewards.reverse()\n",
        "  for r in rewards:\n",
        "    sum_reward = r + gamma*sum_reward\n",
        "    discnt_rewards.append(sum_reward)\n",
        "  discnt_rewards.reverse()  \n",
        "\n",
        "  for state, reward, action in zip(states, discnt_rewards, actions):\n",
        "      with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
        "        log_prob, _ = actor.sample_prob(state, action)\n",
        "\n",
        "        values = critic(state)\n",
        "\n",
        "        advantage = reward - values\n",
        "        actor_loss = -tf.math.reduce_sum(log_prob * advantage)\n",
        "\n",
        "        critic_loss = huber_loss(values, reward)\n",
        "\n",
        "        loss = actor_loss + critic_loss\n",
        "\n",
        "\n",
        "      actor_grads = actor_tape.gradient(actor_loss, actor.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))\n",
        "\n",
        "      critic_grads = critic_tape.gradient(critic_loss, critic.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n"
      ],
      "metadata": {
        "id": "oe3Mt4v9jrjY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}